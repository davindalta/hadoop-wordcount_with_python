{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22f4b074-34e5-4d91-8250-62a38560f898",
   "metadata": {
    "id": "QIsVT46Je21m"
   },
   "source": [
    "#Introduction\n",
    "\n",
    "Hadoop is an open-source framework which is mainly used for storage purposes and maintaining and analyzing a large amount of data or datasets on the clusters of commodity hardware, which means it is actually a data management tool.\n",
    "\n",
    "##Hadoop mainly works on 3 different modes:\n",
    "\n",
    "###Standalone Mode\n",
    "\n",
    "Pseudo-distributed Mode\n",
    "Fully-distributed Mode\n",
    "Standalone Mode\n",
    "\n",
    "By default, Hadoop is configured to run in a non distributed mode. It runs as a single Java process. Instead of HDFS, this mode utilizes the local file system. This mode is useful for debugging and there isn't any need to configure core-site.xml, hdfs-site.xml, mapred-site.xml, masters & slaves. Stand-alone mode is usually the fastest mode in Hadoop.\n",
    "\n",
    "###Pseudo-distributed Mode\n",
    "\n",
    "Hadoop can also run on a single node in a Pseudo-distributed mode. In this mode, each daemon runs on separate java processes. In this mode custom configuration is required (core-site.xml, hdfs-site.xml, mapred-site.xml). Here HDFS is utilized for input and output. This mode of deployment is useful for testing and debugging purposes.\n",
    "\n",
    "###Fully-distributed Mode\n",
    "\n",
    "This is the production mode of Hadoop. In this mode typically one machine in the cluster is designated as NameNode and another as Resource Manager exclusively. These are masters. All other nodes act as Data Node and Node Manager. These are the slaves. Configuration parameters and environment need to be specified for Hadoop Daemons.\n",
    "\n",
    "Installing Java 8\n",
    "Hadoop is a java programming-based data processing framework\n",
    "\n",
    "OpenJDK is a development environment for building applications, applets, and components using the Java programming language."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295eb74d-3378-491c-aa48-5ae359bf0c53",
   "metadata": {},
   "source": [
    "# Installing Java 8 and Hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd52dbb-dc9f-4188-9bf1-421d9bbcb210",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iinstalling java 8\n",
    "! apt install openjdk-8-jdk -y\n",
    "!java -version\n",
    "\n",
    "!update-alternatives --set java /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java\n",
    "!update-alternatives --set javac /usr/lib/jvm/java-8-openjdk-amd64/bin/javac\n",
    "!update-alternatives --set jps /usr/lib/jvm/java-8-openjdk-amd64/bin/jps\n",
    "!java -version\n",
    "\n",
    "# Installing openSSH on Ubuntu\n",
    "# Install the OpenSSH server and client using the following command:\n",
    "! apt install openssh-server openssh-client -y\n",
    "\n",
    "#Creating a new rsa key pair with empty password\n",
    "!ssh-keygen -t rsa -P \"\" -f ~/.ssh/id_rsa \n",
    "\n",
    "# store the public key as authorized_keys in the ssh directory\n",
    "!cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys\n",
    "\n",
    "#Changing the permissions on the key\n",
    "!chmod 0600 ~/.ssh/authorized_keys\n",
    "\n",
    "#Conneting with the local machine\n",
    "!ssh -o StrictHostKeyChecking=no localhost uptime\n",
    "\n",
    "#Downloading Hadoop 3.2.3\n",
    "!wget -q https://archive.apache.org/dist/hadoop/common/hadoop-3.2.3/hadoop-3.2.3.tar.gz\n",
    "\n",
    "#Untarring the file\n",
    "!sudo tar -xzf hadoop-3.2.3.tar.gz\n",
    "#Removing the tar file\n",
    "!rm hadoop-3.2.3.tar.gz\n",
    "\n",
    "\n",
    "#Copying the hadoop files to user/local\n",
    "!cp -r hadoop-3.2.3/ /usr/local/\n",
    "#-r copy directories recursively\n",
    "\n",
    "#Adding JAVA_HOME directory to hadoop-env.sh file\n",
    "!sed -i '/export JAVA_HOME=/a export JAVA_HOME=\\/usr\\/lib\\/jvm\\/java-8-openjdk-amd64' /usr/local/hadoop-3.2.3/etc/hadoop/hadoop-env.sh\n",
    "\n",
    "# Configure Hadoop Environment Variables (bashrc)\n",
    "# Edit the .bashrc shell configuration file using a text editor (nano)\n",
    "!nano .bashrc\n",
    "\n",
    "# Define the Hadoop environment variables by adding the following content to the end of the file\n",
    "# Hadoop Related Options\n",
    "export HADOOP_HOME=/usr/local/hadoop-3.2.3\n",
    "export HADOOP_INSTALL=$HADOOP_HOME\n",
    "export HADOOP_MAPRED_HOME=$HADOOP_HOME\n",
    "export HADOOP_COMMON_HOME=$HADOOP_HOME\n",
    "export HADOOP_HDFS_HOME=$HADOOP_HOME\n",
    "export YARN_HOME=$HADOOP_HOME\n",
    "export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native\n",
    "export PATH=$PATH:$HADOOP_HOME/sbin:$HADOOP_HOME/bin\n",
    "export HADOOP_OPTS=\"-Djava.library.path=$HADOOP_HOME/lib/native\"\n",
    "\n",
    "# apply the changes to the current running environment\n",
    "!source .bashrc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82fe6b5-e5ae-4905-a021-b19705c12777",
   "metadata": {},
   "outputs": [],
   "source": [
    "# edit core-site.xml file\n",
    "!nano $HADOOP_HOME/etc/hadoop/core-site.xml\n",
    "\n",
    "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
    "<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n",
    "<!--\n",
    "  Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "  you may not use this file except in compliance with the License.\n",
    "  You may obtain a copy of the License at\n",
    "\n",
    "    http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "  Unless required by applicable law or agreed to in writing, software\n",
    "  distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "  See the License for the specific language governing permissions and\n",
    "  limitations under the License. See accompanying LICENSE file.\n",
    "-->\n",
    "\n",
    "<!-- Put site-specific property overrides in this file. -->\n",
    "<configuration>\n",
    "<property>\n",
    "  <name>hadoop.tmp.dir</name>\n",
    "  <value>/usr/local/hadoop-3.4.3/tmpdata</value>\n",
    "</property>\n",
    "<property>\n",
    "  <name>fs.default.name</name>\n",
    "  <value>hdfs://127.0.0.1:9000</value>\n",
    "</property>\n",
    "</configuration>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f800d54c-32b7-46d9-ac0b-ef9016b7b4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# edit hdfs-site.xml file \n",
    "!sudo nano $HADOOP_HOME/etc/hadoop/hdfs-site.xml\n",
    "\n",
    "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
    "<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n",
    "<!--\n",
    "  Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "  you may not use this file except in compliance with the License.\n",
    "  You may obtain a copy of the License at\n",
    "\n",
    "    http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "  Unless required by applicable law or agreed to in writing, software\n",
    "  distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "  See the License for the specific language governing permissions and\n",
    "  limitations under the License. See accompanying LICENSE file.\n",
    "-->\n",
    "\n",
    "<!-- Put site-specific property overrides in this file. -->\n",
    "<configuration>\n",
    "<property>\n",
    "  <name>dfs.data.dir</name>\n",
    "  <value>/usr/local/hadoop-3.4.3/dfsdata/namenode</value>\n",
    "</property>\n",
    "<property>\n",
    "  <name>dfs.data.dir</name>\n",
    "  <value>/usr/local/hadoop-3.4.3/dfsdata/datanode</value>\n",
    "</property>\n",
    "<property>\n",
    "  <name>dfs.replication</name>\n",
    "  <value>1</value>\n",
    "</property>\n",
    "</configuration>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a927edc5-70c2-4710-a836-287a061f6643",
   "metadata": {},
   "outputs": [],
   "source": [
    "# edit mapred.xml file\n",
    "!nano $HADOOP_HOME/etc/hadoop/mapred-site.xml\n",
    "\n",
    "<?xml version=\"1.0\"?>\n",
    "<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n",
    "<!--\n",
    "  Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "  you may not use this file except in compliance with the License.\n",
    "  You may obtain a copy of the License at\n",
    "\n",
    "    http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "  Unless required by applicable law or agreed to in writing, software\n",
    "  distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "  See the License for the specific language governing permissions and\n",
    "  limitations under the License. See accompanying LICENSE file.\n",
    "-->\n",
    "\n",
    "<!-- Put site-specific property overrides in this file. -->\n",
    "<configuration>\n",
    "<property>\n",
    "  <name>mapreduce.framework.name</name>\n",
    "  <value>yarn</value>\n",
    "</property>\n",
    "</configuration>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b383b994-c062-444b-bd02-899ce20de3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# edit yarn-site.xml file\n",
    "!nano $HADOOP_HOME/etc/hadoop/yarn-site.xml\n",
    "\n",
    "<configuration>\n",
    "<property>\n",
    "  <name>yarn.nodemanager.aux-services</name>\n",
    "  <value>mapreduce_shuffle</value>\n",
    "</property>\n",
    "<property>\n",
    "  <name>yarn.nodemanager.aux-services.mapreduce.shuffle.class</name>\n",
    "  <value>org.apache.hadoop.mapred.ShuffleHandler</value>\n",
    "</property>\n",
    "<property>\n",
    "  <name>yarn.resourcemanager.hostname</name>\n",
    "  <value>127.0.0.1</value>\n",
    "</property>\n",
    "<property>\n",
    "  <name>yarn.acl.enable</name>\n",
    "  <value>0</value>\n",
    "</property>\n",
    "<property>\n",
    "  <name>yarn.nodemanager.env-whitelist</name>   \n",
    "  <value>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PERPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_MAPRED_HOME</value>\n",
    "</property>\n",
    "</configuration>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7fd32f-ae06-443c-bef6-c0279b923ef7",
   "metadata": {},
   "source": [
    "## Formatting the HDFS Filesystem\n",
    "\n",
    "Before HDFS can be used for the first time the file system must be formatted. The formatting process creates an empty file system by creating the storage directories and the initial versions of the NameNodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98a0b93-7e65-4968-a16f-c12ef13c69ee",
   "metadata": {},
   "source": [
    "!$HADOOP_HOME/bin/hdfs namenode -format\n",
    "\n",
    "#Launching hdfs deamons\n",
    "!$HADOOP_HOME/sbin/start-dfs.sh\n",
    "\n",
    "#Launching yarn deamons\n",
    "!$HADOOP_HOME/sbin/start-yarn.sh\n",
    "\n",
    "#Listing the running deamons\n",
    "!jps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea03a81a-de0d-4697-b134-c60526d3ec84",
   "metadata": {},
   "source": [
    "### Monitoring Hadoop cluster with hadoop admin commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3076767f-7e68-4507-b26c-04657f56af9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Report the basic file system information and statistics\n",
    "!$HADOOP_HOME/bin/hdfs dfsadmin -report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f841146a-3380-40eb-af8d-89257ffd28dd",
   "metadata": {},
   "source": [
    "## MapReduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256ee332-6572-4877-9848-685adcac9a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating directory in HDFS\n",
    "!$HADOOP_HOME/bin/hdfs dfs -mkdir /word_count\n",
    "#Coping file from local file system to HDFS\n",
    "!$HADOOP_HOME/bin/hdfs dfs -put /documents/pembukaan-uud1945.txt /word_count\n",
    "\n",
    "#Exploring Hadoop folder\n",
    "!$HADOOP_HOME/bin/hdfs dfs -ls /word_count\n",
    "\n",
    "# Run MapReduce Example using JAVA\n",
    "!$HADOOP_HOME/bin/hadoop jar $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.2.3.jar wordcount /word_count/pembukaan-uud1945.txt /word_count/output/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fea867d-5164-4ad2-b749-de58ec2db9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exploring the created output directory\n",
    "#part-r-00000 contains the actual ouput\n",
    "!$HADOOP_HOME/bin/hdfs dfs -ls /word_count/output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4737d7-1d94-4c39-8973-4dd9b60b8027",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Printing out first 50 lines\n",
    "!$HADOOP_HOME/bin/hdfs dfs -cat /word_count/output/part-r-00000 | head -50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87791be0-4098-40eb-bee9-ab1b93591cdf",
   "metadata": {},
   "source": [
    "# Hadoop Streaming Using Python\n",
    "\n",
    "Hadoop Streaming is a feature that comes with Hadoop and allows users or developers to use various different languages for writing MapReduce programs like Python, C++, Ruby, etc.\n",
    "\n",
    "The utility will create a Map/Reduce job, submit the job to an appropriate cluster, and monitor the progress of the job until it completes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edbea5d6-e01d-4d25-84ca-bae91b288136",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating directory in HDFS\n",
    "!$HADOOP_HOME/bin/hdfs dfs -mkdir /word_count_with_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9caac7a0-e41c-4b3c-8ed0-59dae2d0d605",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Copying the file from local file system to Hadoop distributed file system (HDFS)\n",
    "!$HADOOP_HOME/bin/hdfs dfs -put /documents/pembukaan-uud1945.txt /word_count_with_python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cda3b42-396a-402d-8d0a-9d5256ab7bd6",
   "metadata": {},
   "source": [
    "## Mapper\n",
    "\n",
    "The mapper is an executable that reads all input records from a file/s and generates an output in the form of key-value pairs which works as input for the Reducer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1510005-0db3-4dc8-ad25-a4cd4404df13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open nano text editor and create file mapper.py\n",
    "!nano \n",
    "\n",
    "#!/usr/bin/env python\n",
    "\n",
    "#'#!' is known as shebang and used for interpreting the script\n",
    "\n",
    "# import sys because we need to read and write data to STDIN and STDOUT\n",
    "import sys\n",
    "\n",
    "# reading entire line from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "  # to remove leading and trailing whitespace\n",
    "  ###\n",
    "  ### \"sadsadas dsdasda\"\n",
    "  ### sadsadas\n",
    "  ### sadsadas\n",
    "  line = line.strip()\n",
    "  # split the line into words, output data type list\n",
    "  words = line.split()\n",
    "\n",
    "  # we are looping over the words array and printing the word\n",
    "  # with the count of 1 to the STDOUT\n",
    "  for word in words:\n",
    "    # write the results to STDOUT (standard output);\n",
    "    # what we output here will be the input for the\n",
    "    # Reduce step, i.e. the input for reducer.py\n",
    "    print('%s\\t%s' % (word, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61bd484d-8627-4d4e-9e45-5e457e983117",
   "metadata": {},
   "source": [
    "## Reducer\n",
    "\n",
    "The reducer is an executable that reads all the intermediate key-value pairs generated by the mapper and generates a final output as a result of a computation operation like addition, filtration, and aggregation.\n",
    "\n",
    "Both the mapper and the reducer read the input from stdin (line by line) and emit the output to stdout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8dfd68-648f-493f-8f4f-abbae992e466",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open nano text editor and create file reducer.py\n",
    "!nano\n",
    "\n",
    "#!/usr/bin/env python\n",
    "\n",
    "from operator import itemgetter\n",
    "import sys\n",
    "\n",
    "current_word = None\n",
    "current_count = 0\n",
    "word = None\n",
    "\n",
    "# read the entire line from STDIN\n",
    "for line in sys.stdin:\n",
    "  # remove leading and trailing whitespace\n",
    "  line = line.strip()\n",
    "  # splitting the data on the basis of tab we have provided in mapper.py\n",
    "  word, count = line.split('\\t', 1)\n",
    "  # convert count (currently a string) to int\n",
    "  try:\n",
    "    count = int(count)\n",
    "  except ValueError:\n",
    "    # count was not a number, so silently\n",
    "    # ignore/discard this line\n",
    "    continue\n",
    "\n",
    "  # this IF-switch only works because Hadoop sorts map output\n",
    "  # by key (here: word) before it is passed to the reducer\n",
    "  if current_word == word:\n",
    "    current_count += count\n",
    "  else:\n",
    "    if current_word: #to not print current_word=None\n",
    "      # write result to STDOUT\n",
    "      print('%s\\t%s' % (current_word, current_count))\n",
    "    current_count = count\n",
    "    current_word = word\n",
    "\n",
    "# do not forget to output the last word if needed!\n",
    "if current_word == word:\n",
    "  print('%s\\t%s' % (current_word, current_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01378703-bb14-47df-86ca-5f70b41bb80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Changing the permissions of the files\n",
    "!chmod 777 /documents/mapper.py /documents/reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc41132-ffad-441a-8e3b-95d5eb19a870",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Running MapReduce programs\n",
    "!$HADOOP_HOME/bin/hadoop jar $HADOOP_HOME/share/hadoop/tools/lib/hadoop-streaming-3.2.3.jar \\\n",
    "  -input /word_count_with_python/pembukaan-uud1945.txt \\\n",
    "  -output /word_count_with_python/output \\\n",
    "  -mapper \"python3 /documents/mapper.py\" \\\n",
    "  -reducer \"python3 /documents/reducer.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52e9762-9f86-403c-b41e-4d50695d3e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exploring the created output directory\n",
    "#part-r-00000 contains the actual ouput\n",
    "!$HADOOP_HOME/bin/hdfs dfs -ls /word_count_with_python/output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec363dd-b461-4780-89d7-e86115bd8510",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Printing out first 50 lines\n",
    "!$HADOOP_HOME/bin/hdfs dfs -cat /word_count_with_python/output/part-00000 | head -50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ffe7748-41be-4273-9da9-f658a46a133f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!$HADOOP_HOME/bin/hdfs dfs -copyToLocal /word_count_with_python/output/part-00000 /documents/hdfs-wordcount-pembukaan-uud1945.txt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
